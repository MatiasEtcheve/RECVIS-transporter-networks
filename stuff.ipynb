{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: python: not found\r\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "import mmcv\n",
    "from depth.datasets.pipelines import Compose\n",
    "import torch\n",
    "from mmcv.parallel import MMDataParallel, MMDistributedDataParallel\n",
    "from mmcv.runner import get_dist_info, init_dist, load_checkpoint, wrap_fp16_model\n",
    "from mmcv.utils import DictAction\n",
    "\n",
    "from depth.apis import multi_gpu_test, single_gpu_test\n",
    "from depth.datasets import build_dataloader, build_dataset\n",
    "from depth.models import build_depther\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_to_images = Path(\"../dataset/block-insertion-test/color/000000-1.pkl\")\n",
    "path_to_config = Path(\"configs/binsformer/binsformer_swint_w7_nyu.py\")\n",
    "path_to_checkpoint = Path(\"../checkpoints/binsformer_swint_nyu_converted.pth\")\n",
    "\n",
    "batch = torch.Tensor(pickle.load(open(path_to_images, \"rb\"))[0])\n",
    "image = batch[0]\n",
    "img_metas = [\n",
    "    {\n",
    "        \"pad_shape\": tuple(_.shape),\n",
    "        \"img_shape\": tuple(_.shape),\n",
    "        \"ori_shape\": tuple(_.shape),\n",
    "        \"scale_factor\": 1,\n",
    "        \"img_norm_cfg\": {\n",
    "            \"mean\": _.mean(axis=(0, 1)),\n",
    "            \"std\": _.mean(axis=(0, 1)),\n",
    "        },\n",
    "    }\n",
    "    for _ in batch\n",
    "]\n",
    "# plt.imshow(image)\n",
    "outputs = model.forward([batch.permute((0, 3, 1, 2))], [img_metas], return_loss=False)\n",
    "# ori_shapes = [_['ori_shape'] for _ in img_metas]\n",
    "# print(ori_shapes, [shape == ori_shapes[0] for shape in ori_shapes])\n",
    "# assert all(shape == ori_shapes[0] for shape in ori_shapes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transpose() received an invalid combination of arguments - got (Tensor, tuple, tuple), but expected one of:\n * (Tensor input, name dim0, name dim1)\n      didn't match because some of the arguments have invalid types: (Tensor, !tuple!, !tuple!)\n * (Tensor input, int dim0, int dim1)\n      didn't match because some of the arguments have invalid types: (Tensor, !tuple!, !tuple!)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_10192/3389900881.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m: transpose() received an invalid combination of arguments - got (Tensor, tuple, tuple), but expected one of:\n * (Tensor input, name dim0, name dim1)\n      didn't match because some of the arguments have invalid types: (Tensor, !tuple!, !tuple!)\n * (Tensor input, int dim0, int dim1)\n      didn't match because some of the arguments have invalid types: (Tensor, !tuple!, !tuple!)\n"
     ]
    }
   ],
   "source": [
    "torch.transpose(batch, (0, 1, 2, 3), (0, 3, 1, 2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "Compose(\n    LoadImageFromFile(to_float32=False,color_type='color',imdecode_backend='cv2')\n    RandomFlip(prob=0.0)\n    Normalize(mean=[123.675 116.28  103.53 ], std=[58.395 57.12  57.375], to_rgb=True)\n    ImageToTensor(keys=['img'])\n    Collect(keys=['img'], meta_keys=('filename', 'ori_filename', 'ori_shape', 'img_shape', 'pad_shape', 'scale_factor', 'flip', 'flip_direction', 'img_norm_cfg', 'cam_intrinsic'))\n)"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_norm_cfg = dict(\n",
    "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True\n",
    ")\n",
    "\n",
    "eval_pipeline = [\n",
    "    dict(type=\"LoadImageFromFile\"),\n",
    "    dict(type=\"RandomFlip\", prob=0.0),  # set to zero\n",
    "    dict(type=\"Normalize\", **img_norm_cfg),\n",
    "    dict(type=\"ImageToTensor\", keys=[\"img\"]),\n",
    "    dict(\n",
    "        type=\"Collect\",\n",
    "        keys=[\"img\"],\n",
    "        meta_keys=(\n",
    "            \"filename\",\n",
    "            \"ori_filename\",\n",
    "            \"ori_shape\",\n",
    "            \"img_shape\",\n",
    "            \"pad_shape\",\n",
    "            \"scale_factor\",\n",
    "            \"flip\",\n",
    "            \"flip_direction\",\n",
    "            \"img_norm_cfg\",\n",
    "            \"cam_intrinsic\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "Compose(eval_pipeline)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth here {'type': 'NYUBinFormerDataset', 'data_root': 'data/nyu/', 'depth_scale': 1000, 'split': 'nyu_test.txt', 'pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'MultiScaleFlipAug', 'img_scale': (480, 640), 'flip': True, 'flip_direction': 'horizontal', 'transforms': [{'type': 'RandomFlip', 'direction': 'horizontal'}, {'type': 'Normalize', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'ImageToTensor', 'keys': ['img']}, {'type': 'Collect', 'keys': ['img']}]}], 'garg_crop': False, 'eigen_crop': True, 'min_depth': 0.001, 'max_depth': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/RECVIS/RECVIS-transporter-networks/Monocular-Depth-Estimation-Toolbox/depth/models/backbones/swin.py:612: UserWarning: DeprecationWarning: pretrained is a deprecated, please use \"init_cfg\" instead\n",
      "  warnings.warn('DeprecationWarning: pretrained is a deprecated, '\n",
      "/home/matias/.pyenv/versions/3.7.12/envs/RECVIS/lib/python3.7/site-packages/mmcv/cnn/bricks/transformer.py:342: UserWarning: The arguments `feedforward_channels` in BaseTransformerLayer has been deprecated, now you should set `feedforward_channels` and other FFN related arguments to a dict named `ffn_cfgs`. \n",
      "  f'The arguments `{ori_name}` in BaseTransformerLayer '\n",
      "/home/matias/.pyenv/versions/3.7.12/envs/RECVIS/lib/python3.7/site-packages/mmcv/cnn/bricks/transformer.py:342: UserWarning: The arguments `ffn_dropout` in BaseTransformerLayer has been deprecated, now you should set `ffn_drop` and other FFN related arguments to a dict named `ffn_cfgs`. \n",
      "  f'The arguments `{ori_name}` in BaseTransformerLayer '\n",
      "/home/matias/.pyenv/versions/3.7.12/envs/RECVIS/lib/python3.7/site-packages/mmcv/cnn/bricks/transformer.py:92: UserWarning: The arguments `dropout` in MultiheadAttention has been deprecated, now you can separately set `attn_drop`(float), proj_drop(float), and `dropout_layer`(dict) \n",
      "  warnings.warn('The arguments `dropout` in MultiheadAttention '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: ../checkpoints/binsformer_swint_nyu_converted.pth\n"
     ]
    }
   ],
   "source": [
    "cfg = mmcv.Config.fromfile(path_to_config)\n",
    "print(cfg.model.pretrained, \"here\", cfg.data.test)\n",
    "# cfg.model.pretrained = None\n",
    "# cfg.data.test.test_mode = True\n",
    "# pprint(cfg._cfg_dict)\n",
    "model = build_depther(cfg.model, test_cfg=cfg.get(\"test_cfg\"))\n",
    "checkpoint = load_checkpoint(model, str(path_to_checkpoint), map_location=\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pad_shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_10192/1114772099.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mimg_metas\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturn_loss\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.pyenv/versions/3.7.12/envs/RECVIS/lib/python3.7/site-packages/mmcv/runner/fp16_utils.py\u001B[0m in \u001B[0;36mnew_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     96\u001B[0m                                 'method of nn.Module')\n\u001B[1;32m     97\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'fp16_enabled'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfp16_enabled\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 98\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mold_func\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     99\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m             \u001B[0;31m# get the arg spec of the decorated method\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/RECVIS/RECVIS-transporter-networks/Monocular-Depth-Estimation-Toolbox/depth/models/depther/base.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, img, img_metas, return_loss, **kwargs)\u001B[0m\n\u001B[1;32m    109\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimg_metas\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 111\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward_test\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimg_metas\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtrain_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_batch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/RECVIS/RECVIS-transporter-networks/Monocular-Depth-Estimation-Toolbox/depth/models/depther/base.py\u001B[0m in \u001B[0;36mforward_test\u001B[0;34m(self, imgs, img_metas, **kwargs)\u001B[0m\n\u001B[1;32m     87\u001B[0m             \u001B[0mimg_shapes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0m_\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'img_shape'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mimg_meta\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     88\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshape\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mimg_shapes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mshape\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mimg_shapes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 89\u001B[0;31m             \u001B[0mpad_shapes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0m_\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'pad_shape'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mimg_meta\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     90\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshape\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mpad_shapes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mshape\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpad_shapes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     91\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/RECVIS/RECVIS-transporter-networks/Monocular-Depth-Estimation-Toolbox/depth/models/depther/base.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     87\u001B[0m             \u001B[0mimg_shapes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0m_\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'img_shape'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mimg_meta\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     88\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshape\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mimg_shapes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mshape\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mimg_shapes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 89\u001B[0;31m             \u001B[0mpad_shapes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0m_\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'pad_shape'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mimg_meta\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     90\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshape\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mpad_shapes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mshape\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpad_shapes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     91\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'pad_shape'"
     ]
    }
   ],
   "source": [
    "model.forward([batch], [img_metas], return_loss=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
