{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import os.path as osp\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.image\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import torch\n",
    "from depth import __version__\n",
    "from depth.apis import set_random_seed, train_depther\n",
    "from depth.datasets import build_dataset\n",
    "from depth.models import build_depther\n",
    "from depth.utils import collect_env, get_root_logger\n",
    "from mmcv.runner import init_dist\n",
    "from mmcv.utils import Config, DictAction, get_git_hash\n",
    "from tqdm import tqdm\n",
    "\n",
    "# matplotlib.image.imsave('name.png', array)\n",
    "# im = Image.fromarray(data[0, 0], mode=\"F\").convert(\"RGB\")\n",
    "# im.save(f\"{fname.split('.')[0]}.jpeg\")\n",
    "\n",
    "# ├── data\n",
    "# │   ├── custom\n",
    "# │   │   ├── train\n",
    "# │   │   │   ├── rgb\n",
    "# │   │   │   │   ├── 0.xxx\n",
    "# │   │   │   │   ├── 1.xxx\n",
    "# │   │   │   │   ├── 2.xxx\n",
    "# │   │   │   ├── depth\n",
    "# │   │   │   │   ├── 0.xxx\n",
    "# │   │   │   │   ├── 1.xxx\n",
    "# │   │   │   │   ├── 2.xxx\n",
    "# │   │   ├── val\n",
    "# │   │   │   ...\n",
    "# │   │   │   ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tasks found: ['block-insertion' 'manipulating-rope']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [06:11<00:00,  2.69it/s]\n",
      "100%|██████████| 100/100 [00:32<00:00,  3.12it/s]\n",
      "100%|██████████| 1000/1000 [25:55<00:00,  1.56s/it]\n",
      "100%|██████████| 100/100 [02:38<00:00,  1.58s/it]\n"
     ]
    }
   ],
   "source": [
    "data_root = Path(\"../dataset/depth\")\n",
    "\n",
    "\n",
    "def transform_task_dataset_to_images(task_dataset_root, image_dataset_root):\n",
    "    all_subdirectories = [f.stem for f in task_dataset_root.iterdir() if f.is_dir()]\n",
    "    task_names = np.unique(\n",
    "        [\n",
    "            \"-\".join(f.split(\"-\")[:2])\n",
    "            for f in all_subdirectories\n",
    "            if \"train\" in f or \"test\" in f\n",
    "        ]\n",
    "    )\n",
    "    print(f\"{len(task_names)} tasks found: {task_names}\")\n",
    "    for task_name in task_names:\n",
    "        for _mode in [\"train\", \"test\"]:\n",
    "            mode = _mode if _mode == \"train\" else \"val\"\n",
    "            dataset_root = task_dataset_root / f\"{task_name}-{_mode}/\"\n",
    "\n",
    "            color_files = sorted(\n",
    "                [f for f in (dataset_root / \"color\").glob(\"**/*\") if f.is_file()]\n",
    "            )\n",
    "            depth_files = sorted(\n",
    "                [f for f in (dataset_root / \"depth\").glob(\"**/*\") if f.is_file()]\n",
    "            )\n",
    "\n",
    "            for color_file, depth_file in tqdm(\n",
    "                zip(color_files, depth_files), total=len(color_files)\n",
    "            ):\n",
    "                color_images = pickle.load(open(color_file, \"rb\"))\n",
    "                depth_images = pickle.load(open(depth_file, \"rb\"))\n",
    "                color_images_shape = color_images.shape[-3:]\n",
    "\n",
    "                color_images = color_images.reshape((-1, *color_images_shape))\n",
    "                depth_images = depth_images.reshape((-1, *color_images_shape[:-1]))\n",
    "\n",
    "                nb_image_per_color_files = len(color_images)\n",
    "                nb_image_per_depth_files = len(depth_images)\n",
    "                assert nb_image_per_color_files == nb_image_per_depth_files\n",
    "\n",
    "                for index, (color_image, depth_image) in enumerate(\n",
    "                    zip(color_images, depth_images)\n",
    "                ):\n",
    "                    filename_color = color_file.stem\n",
    "                    filename_depth = depth_file.stem\n",
    "                    assert (\n",
    "                        filename_color == filename_depth\n",
    "                    ), f\"got 2 different filenames: {filename_color} and {filename_depth}\"\n",
    "                    (image_dataset_root / mode / \"rgb\").mkdir(\n",
    "                        parents=True, exist_ok=True\n",
    "                    )\n",
    "                    (image_dataset_root / mode / \"depth\").mkdir(\n",
    "                        parents=True, exist_ok=True\n",
    "                    )\n",
    "                    matplotlib.image.imsave(\n",
    "                        image_dataset_root\n",
    "                        / mode\n",
    "                        / \"rgb\"\n",
    "                        / (filename_color.split(\"-\")[0] + f\"-{task_name}-{index}.png\"),\n",
    "                        color_image,\n",
    "                    )\n",
    "                    matplotlib.image.imsave(\n",
    "                        image_dataset_root\n",
    "                        / mode\n",
    "                        / \"depth\"\n",
    "                        / (filename_color.split(\"-\")[0] + f\"-{task_name}-{index}.png\"),\n",
    "                        depth_image,\n",
    "                    )\n",
    "\n",
    "\n",
    "transform_task_dataset_to_images(Path(\"dataset\"), Path(\"dataset/depth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"depth_custom_dataset.py\"\n",
    "work_dir = None\n",
    "load_from = \"checkpoints/binsformer_swint_nyu_converted.pth\"\n",
    "resume_from = None\n",
    "no_validate = False\n",
    "gpus = None\n",
    "gpu_ids = None\n",
    "seed = 42\n",
    "deterministic = True\n",
    "options = None\n",
    "launcher = \"none\"\n",
    "local_rank = 0\n",
    "\n",
    "if 'LOCAL_RANK' not in os.environ:\n",
    "    os.environ['LOCAL_RANK'] = str(local_rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-30 17:26:23,704 - depth - INFO - Environment info:\n",
      "------------------------------------------------------------\n",
      "sys.platform: linux\n",
      "Python: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) [GCC 9.4.0]\n",
      "CUDA available: False\n",
      "GCC: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\n",
      "PyTorch: 1.8.0\n",
      "PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
      "\n",
      "TorchVision: 0.9.0\n",
      "OpenCV: 4.7.0\n",
      "MMCV: 1.5.0\n",
      "MMCV Compiler: GCC 7.3\n",
      "MMCV CUDA Compiler: 10.2\n",
      "Depth: 0.1.1+9b017a2\n",
      "------------------------------------------------------------\n",
      "\n",
      "2022-12-30 17:26:23,705 - depth - INFO - Distributed training: False\n",
      "2022-12-30 17:26:23,835 - depth - INFO - Config:\n",
      "dataset_type = 'CustomDepthDataset'\n",
      "data_root = 'dataset/depth/'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "crop_size = (416, 544)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='DepthLoadAnnotations'),\n",
      "    dict(type='NYUCrop', depth=True),\n",
      "    dict(type='RandomRotate', prob=0.5, degree=2.5),\n",
      "    dict(type='RandomFlip', prob=0.5),\n",
      "    dict(type='RandomCrop', crop_size=(416, 544)),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'depth_gt'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='Resize', img_scale=(480, 640)),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(0, 0),\n",
      "        flip=True,\n",
      "        flip_direction='horizontal',\n",
      "        transforms=[\n",
      "            dict(type='RandomFlip', direction='horizontal'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=16,\n",
      "    workers_per_gpu=8,\n",
      "    train=dict(\n",
      "        type='CustomDepthDataset',\n",
      "        data_root='dataset/depth//train',\n",
      "        depth_scale=1,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='DepthLoadAnnotations'),\n",
      "            dict(type='NYUCrop', depth=True),\n",
      "            dict(type='RandomRotate', prob=0.5, degree=2.5),\n",
      "            dict(type='RandomFlip', prob=0.5),\n",
      "            dict(type='RandomCrop', crop_size=(416, 544)),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(type='Collect', keys=['img', 'depth_gt'])\n",
      "        ],\n",
      "        min_depth=0.001,\n",
      "        max_depth=10),\n",
      "    val=dict(\n",
      "        type='CustomDepthDataset',\n",
      "        data_root='dataset/depth//val',\n",
      "        depth_scale=1,\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='Resize', img_scale=(480, 640)),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(0, 0),\n",
      "                flip=True,\n",
      "                flip_direction='horizontal',\n",
      "                transforms=[\n",
      "                    dict(type='RandomFlip', direction='horizontal'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        min_depth=0.001,\n",
      "        max_depth=10),\n",
      "    test=dict(\n",
      "        type='CustomDepthDataset',\n",
      "        data_root='dataset/depth//val',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='Resize', img_scale=(480, 640)),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(0, 0),\n",
      "                flip=True,\n",
      "                flip_direction='horizontal',\n",
      "                transforms=[\n",
      "                    dict(type='RandomFlip', direction='horizontal'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        min_depth=0.001,\n",
      "        max_depth=10))\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "workflow = [('train', 1)]\n",
      "max_lr = 0.0001\n",
      "optimizer = dict(\n",
      "    type='AdamW',\n",
      "    lr=0.0001,\n",
      "    betas=(0.9, 0.999),\n",
      "    weight_decay=0.01,\n",
      "    paramwise_cfg=dict(\n",
      "        custom_keys=dict(\n",
      "            absolute_pos_embed=dict(decay_mult=0.0),\n",
      "            relative_position_bias_table=dict(decay_mult=0.0),\n",
      "            norm=dict(decay_mult=0.0))))\n",
      "lr_config = dict(\n",
      "    policy='OneCycle',\n",
      "    max_lr=0.0001,\n",
      "    warmup_iters=12800,\n",
      "    div_factor=25,\n",
      "    final_div_factor=100,\n",
      "    by_epoch=False)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n",
      "runner = dict(type='IterBasedRunner', max_iters=38400)\n",
      "checkpoint_config = dict(by_epoch=False, max_keep_ckpts=2, interval=1600)\n",
      "evaluation = dict(\n",
      "    by_epoch=False,\n",
      "    start=0,\n",
      "    interval=1600,\n",
      "    pre_eval=True,\n",
      "    rule='less',\n",
      "    save_best='abs_rel',\n",
      "    greater_keys=('a1', 'a2', 'a3'),\n",
      "    less_keys=('abs_rel', 'rmse'))\n",
      "log_config = dict(\n",
      "    _delete_=True,\n",
      "    interval=50,\n",
      "    hooks=[\n",
      "        dict(type='TextLoggerHook', by_epoch=False),\n",
      "        dict(type='TensorboardLoggerHook')\n",
      "    ])\n",
      "find_unused_parameters = True\n",
      "work_dir = './work_dirs/depth_custom_dataset'\n",
      "load_from = 'checkpoints/binsformer_swint_nyu_converted.pth'\n",
      "gpu_ids = range(0, 1)\n",
      "\n",
      "2022-12-30 17:26:23,836 - depth - INFO - Set random seed to 42, deterministic: True\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConfigDict' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14338/737760929.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m model = build_depther(\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mtrain_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_cfg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     test_cfg=cfg.get('test_cfg'))\n",
      "\u001b[0;32m~/miniconda3/envs/RECVIS/lib/python3.7/site-packages/mmcv/utils/config.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cfg_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RECVIS/lib/python3.7/site-packages/mmcv/utils/config.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConfigDict' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "cfg = Config.fromfile(config)\n",
    "if options is not None:\n",
    "    cfg.merge_from_dict(options)\n",
    "# set cudnn_benchmark\n",
    "if cfg.get('cudnn_benchmark', False):\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# work_dir is determined in this priority: CLI > segment in file > filename\n",
    "if work_dir is not None:\n",
    "    # update configs according to CLI args if work_dir is not None\n",
    "    cfg.work_dir = work_dir\n",
    "elif cfg.get('work_dir', None) is None:\n",
    "    # use config filename as default work_dir if cfg.work_dir is None\n",
    "    cfg.work_dir = osp.join('./work_dirs',\n",
    "                            osp.splitext(osp.basename(config))[0])\n",
    "if load_from is not None:\n",
    "    cfg.load_from = load_from\n",
    "if resume_from is not None:\n",
    "    cfg.resume_from = resume_from\n",
    "if gpu_ids is not None:\n",
    "    cfg.gpu_ids = gpu_ids\n",
    "else:\n",
    "    cfg.gpu_ids = range(1) if gpus is None else range(gpus)\n",
    "\n",
    "# init distributed env first, since logger depends on the dist info.\n",
    "if launcher == 'none':\n",
    "    distributed = False\n",
    "# else:\n",
    "#     distributed = True\n",
    "#     init_dist(launcher, **cfg.dist_params)\n",
    "\n",
    "# create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "# dump config\n",
    "cfg.dump(osp.join(cfg.work_dir, osp.basename(config)))\n",
    "# init the logger before other steps\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "log_file = osp.join(cfg.work_dir, f'{timestamp}.log')\n",
    "logger = get_root_logger(log_file=log_file, log_level=cfg.log_level)\n",
    "\n",
    "# init the meta dict to record some important information such as\n",
    "# environment info and seed, which will be logged\n",
    "meta = dict()\n",
    "# log env info\n",
    "env_info_dict = collect_env()\n",
    "env_info = '\\n'.join([f'{k}: {v}' for k, v in env_info_dict.items()])\n",
    "dash_line = '-' * 60 + '\\n'\n",
    "logger.info('Environment info:\\n' + dash_line + env_info + '\\n' +\n",
    "            dash_line)\n",
    "meta['env_info'] = env_info\n",
    "\n",
    "# log some basic info\n",
    "logger.info(f'Distributed training: {distributed}')\n",
    "logger.info(f'Config:\\n{cfg.pretty_text}')\n",
    "\n",
    "# set random seeds\n",
    "if seed is not None:\n",
    "    logger.info(f'Set random seed to {seed}, deterministic: '\n",
    "                f'{deterministic}')\n",
    "    set_random_seed(seed, deterministic=deterministic)\n",
    "cfg.seed = seed\n",
    "meta['seed'] = seed\n",
    "meta['exp_name'] = osp.basename(config)\n",
    "\n",
    "model = build_depther(\n",
    "    cfg.model,\n",
    "    train_cfg=cfg.get('train_cfg'),\n",
    "    test_cfg=cfg.get('test_cfg'))\n",
    "model.init_weights()\n",
    "\n",
    "# NOTE: set all the bn to syncbn\n",
    "import torch.nn as nn\n",
    "if cfg.get('SyncBN', False):\n",
    "    model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "\n",
    "logger.info(model)\n",
    "\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "if len(cfg.workflow) == 2:\n",
    "    val_dataset = copy.deepcopy(cfg.data.val)\n",
    "    val_dataset.pipeline = cfg.data.train.pipeline\n",
    "    datasets.append(build_dataset(val_dataset))\n",
    "if cfg.checkpoint_config is not None:\n",
    "    # save depth version, config file content and class names in\n",
    "    # checkpoints as meta data\n",
    "    cfg.checkpoint_config.meta = dict(\n",
    "        depth_version=f'{__version__}+{get_git_hash()[:7]}',\n",
    "        config=cfg.pretty_text)\n",
    "# passing checkpoint meta for saving best checkpoint\n",
    "meta.update(cfg.checkpoint_config.meta)\n",
    "train_depther(\n",
    "    model,\n",
    "    datasets,\n",
    "    cfg,\n",
    "    distributed=distributed,\n",
    "    validate=(not no_validate),\n",
    "    timestamp=timestamp,\n",
    "    meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
